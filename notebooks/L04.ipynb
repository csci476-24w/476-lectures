{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3109b7e-0092-47a3-aca2-288e745731ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0357b50-2962-423e-85f1-80217ec3d134",
   "metadata": {},
   "source": [
    "#### Announcements\n",
    "\n",
    "* Faculty candidate talks coming up this week!\n",
    "    * Michael Walker (VR/AR for Human Robot Interaction)\n",
    "      * Today (Tuesday 1/23 4pm CF 316 Teaching Demo: Motion Planning for Mobile Robots\n",
    "    * TszChiu Au (Multirobot Systems, from a seemingly mathy angle)\n",
    "      * Thursday 1/25 4pm CF 105: Research Talk\n",
    "      * Friday 1/26 4pm CF 316: Teaching Demo\n",
    "\n",
    "* Calendar adjustments have been made due to the snow day. Most things are just 1 class day later.\n",
    "    * The midterm exam has been pushed back from Tuesday 2/13 to Thursday 2/15.\n",
    "    * I may (no promises) try to compress the topic schedule a little for the few days after the midterm to give more time for the projects and advanced ML topics at the end.\n",
    "* I'm contemplating making the second half of Thursday a mini-lab. Who can bring a laptop (and have the lecture notebook repo cloned, jupyter set up, etc. ahead of class)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efd636-c09e-4d1f-a828-33e0fcc75686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Goals\n",
    "* Know how and why to construct a Gaussian Pyramid\n",
    "* Know how and why to construct a Laplacian Pyramid\n",
    "* Be prepared to complete Project 1\n",
    "\n",
    "---\n",
    "(as time allows)\n",
    "* Understand the high-level steps in a panorama stitching system\n",
    "* Understand the motivation and high-level steps in image matching: feature detection, description, and matching\n",
    "* Be able to describe examples of *global* and *local* image features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1f8e9-e8f6-49c2-bcbb-b0f5e66ce44c",
   "metadata": {},
   "source": [
    "(if we have time?)\n",
    "Introduce our running long-term goal of building a panorama stitching system\n",
    "* Overview\n",
    "* Feature detection and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a295613-e6e3-4f35-b603-27b158385acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if (src_path not in sys.path):\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Library imports\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as skim\n",
    "import cv2\n",
    "\n",
    "# codebase imports\n",
    "import util\n",
    "import filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488e742-09b7-4454-9b18-59e69022fed3",
   "metadata": {},
   "source": [
    "#### Edge Detection: Revisited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59449a2-c06c-4587-8fb3-1696f59bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans = imageio.imread(\"../data/beans.jpg\")\n",
    "beans_small = skim.transform.rescale(beans, 0.25, anti_aliasing=True) # smaller beans\n",
    "bg = skim.color.rgb2gray(beans) # grayscale beans\n",
    "\n",
    "bn = bg + np.random.randn(*bg.shape) * 0.05 # grayscale noisy beans\n",
    "plt.imshow(bn, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcb5ce-f6c6-4595-8073-d83d49f7e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at filtering.grad_mag\n",
    "beans_gradient = filtering.grad_mag(bn)\n",
    "util.imshow_gray(beans_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d24b4-0488-4b37-b73b-25aba358335f",
   "metadata": {},
   "source": [
    "If we fed this into Canny or somesuch, would it give us \"good\" edges? \n",
    "\n",
    "Try zooming in a bit on the edges of her head. You'd think those would be important edges... \n",
    "\n",
    "How strong is the gradient magnitude there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978712f1-6293-4561-a478-622e5e01b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.imshow_gray(beans_gradient[200:350, 50:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fdb46-d3ef-4876-b58c-e56a7bb650fa",
   "metadata": {},
   "source": [
    "Issue: what we conceptualize as edges exist at different spatial scales (or *frequencies*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f9fc5-1494-450a-9cb6-43beba6cd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_small = filtering.down_4x(bn)\n",
    "plt.imshow(filtering.grad_mag(bn_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68275da3-d87e-422c-8c23-cae6d134d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.imshow_gray(cv2.Canny((beans_small * 255).astype(np.uint8), 150, 230))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072be97-3f24-4493-806c-75a7c12f2dbc",
   "metadata": {},
   "source": [
    "So you want to take a derivative at a different scale... how should we go about it?\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de055f9e-e5f4-4f15-a733-86cb53c8102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20448d-1fa0-4db7-9dc7-d3b6e2d81971",
   "metadata": {},
   "source": [
    "##### Homework Problem 1\n",
    "Suppose you want to detect edges at a 2x reduced spatial scale. You have two choices: double the size of your gradient filters, or halve the size of your image (in both height and width). Calculate and compare the number of multiplications required per input pixel to perform just the filtering steps in each of these approaches. Assume that your downsampling prefilter is 5x5 and a \"double size\" sobel filter would end up being 7x7. Which approach will be more efficient if you want to detect edges at multiple reduced spatial scales?\n",
    "\n",
    "For now: which of these do you *think* will be more efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440286b2-3344-4b1a-a280-cc1a7bbe2151",
   "metadata": {},
   "source": [
    "### Gaussian Pyramid\n",
    "\n",
    "Idea: recursively downsample (recall: blur-then-subsample!) to create a multi-scale **image pyramid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05862d9e-ab46-45b9-a7fe-94a27f8e94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = [bg]\n",
    "levels = 6\n",
    "for i in range(levels):\n",
    "    G.append(filtering.down_2x(G[-1]))\n",
    "print(len(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364635a8-3c70-4e4c-a456-7a97e23017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axs = plt.subplots(1, len(G))\n",
    "for i, lvl in enumerate(G):\n",
    "    util.imshow_truesize(lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc4a5e-1a18-4b94-a006-4addbaea837c",
   "metadata": {},
   "source": [
    "#### Frequency Content in the Gaussian Pyramid\n",
    "\n",
    "Each level contains a subset of the original image's frequencies, with the cutoff getting lower at each level.\n",
    "\n",
    "Frequencyometer illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf70072-97e5-4dfe-9313-2e738459d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lvl in enumerate(G):\n",
    "    fig = plt.figure(figsize=(3, 5))\n",
    "    fig.gca().set_axis_off()\n",
    "    util.imshow_gray(lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfbe19-8553-4a0b-b8bb-3c4c0f2878b5",
   "metadata": {},
   "source": [
    "##### Homework Problem 2\n",
    "\n",
    "Suppose you have a 128x128 image, and you compute a full Gaussian pyramid (i.e., every possible level down to a 1x1 image). What multiple of the original image storage is required to store the entire pyramid?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b9c9a-cb2f-41b2-90eb-d8c715fe6475",
   "metadata": {},
   "source": [
    "### Laplacian Pyramids\n",
    "\n",
    "Thinking back to sharpening: what is **lost** from one level of the pyramid to the next?\n",
    "\n",
    "$G_{i+1} = \\textrm{subsample}(\\textrm{blur}(G_i))$\n",
    "\n",
    "Each level of a **Laplacian** pyramid captures this:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_i &= G_i - \\textrm{blur}(G_i) \\textrm{ if } i < k\\\\\n",
    "L_k &= G_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $k$ is index of the highest level (number of levels minus one, since we've zero-indexed).\n",
    "\n",
    "**Ponder**: why should we have the special case for the last (highest, lowest-resolution) level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67614bd-dc48-4e05-ae7d-66c3e0103abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filtering_future import construct_laplacian # you'll write this yourself in P1!\n",
    "L, G = construct_laplacian(filtering.down_2x(bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51538a32-2e7a-4aaf-9816-503330faedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, len(G))\n",
    "for i, lvl in enumerate(L):\n",
    "    util.imshow_truesize(lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2d67c-b69e-41e5-bc97-2e2948bb1f60",
   "metadata": {},
   "source": [
    "#### Frequency Content in the Laplacian Pyramid\n",
    "\n",
    "Frequencyometer illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673414b-28c3-433c-9263-0e45b5524198",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lvl in enumerate(L):\n",
    "    fig = plt.figure(figsize=(3, 5))\n",
    "    fig.gca().set_axis_off()\n",
    "    util.imshow_gray(lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2566645-50d2-4a16-9e54-95d6c3b00072",
   "metadata": {},
   "source": [
    "##### Homework Problems 3-5\n",
    "\n",
    "Suppose you are given the Laplacian and Gaussian pyramids for an input image $I.$ $G_{0 \\ldots k}$ are the Gaussian pyramid levels starting at the original image ($G_0$), while the Laplacian layers $L_{0\\ldots k}$ are the \"detail\" layers, with each $L_i$  at the same resolution as $G_i$.\n",
    "\n",
    "(3) When is it the case that $G_\\ell = L_\\ell$?\n",
    "\n",
    "(4) Given all levels of both pyramids, give an expression that yields a result as close as possible to $G_j$ with a **sharpening** filter applied. You don't need to actually do any filtering.\n",
    "\n",
    "(5) Give an algorithm to reconstruct $G_0$ using **only** the levels of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3ca56-e860-4c78-aae6-82c9247c4dd7",
   "metadata": {},
   "source": [
    "## Project 1 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e2c05-2cc1-4175-8fa8-cb1e8b0ddff6",
   "metadata": {},
   "source": [
    "`python hybrid_gui.py -t resources/sample-correspondence.json -c resources/sample-config.json`\n",
    "\n",
    "`python laplacian_gui.py --image resources/beans.jpg --levels 4`\n",
    "\n",
    "`python local_laplacian_gui.py --image resources/flower-square.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde2264-a4f7-4d81-83e1-c01e92ae9091",
   "metadata": {},
   "source": [
    "#### Tips on numpy efficiency\n",
    "\n",
    "* Loops are slow. Get rid of them wherever possible!\n",
    "    * Example: remove 2 for loops from filtering.filter\n",
    "    * Full efficiency points on P1 requires removing all loops over the image pixels!\n",
    "* General advice: batch as many operations that are happening anyway into a single operation.\n",
    "* There is usually a numpy function to do that thing you're trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc62317-7366-427b-951f-64c112a74035",
   "metadata": {},
   "source": [
    "## Moving on to the next thing...\n",
    "\n",
    "We're going to switch gears a little and introduce a long-term motivating application that will end up being Project 2: \n",
    "\n",
    "# Panorama Stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8980b-dc32-4bf8-94ec-d413ff869e80",
   "metadata": {},
   "source": [
    "The high-level steps for panorama stitching are as follows - see the [slides](../slides/L04_pano_intro.pdf) for more detail and visuals.\n",
    "\n",
    "1. Figure out the geometric relationship between images\n",
    "2. Align images to each other\n",
    "3. Blend them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b65b6-0dfb-48b0-8458-7ebf1c71c62a",
   "metadata": {},
   "source": [
    "Starting with step 1, we have a few sub-steps:\n",
    "1. Figure out the geometric relationship between images\n",
    "   \n",
    "   (a) Identify matching points in neighboring images\n",
    "   \n",
    "   (b) Model (and estimate) the geometric mapping from one point set to another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10931553-8b7d-4503-ae6b-3fcca70087e1",
   "metadata": {},
   "source": [
    "Step 1 (a) itself has some sub-steps:\n",
    "1. (a) Identify matching points in neighboring images\n",
    "    * Find points that would be good to match\n",
    "    * Extract a \"descriptor\" that captures local image information\n",
    "    * Find matching pairs of features using their descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13407878-7a11-4889-922c-e8d526c0b133",
   "metadata": {},
   "source": [
    "... and this brings us to our starting point for next class: **what features should we try to match**?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
