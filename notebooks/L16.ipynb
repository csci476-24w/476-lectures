{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ1zvCUxhz_Y"
   },
   "source": [
    "# Convolutional Layers and Convolutional Networks: A Quick Primer\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are neural networks where some (many) of the layers perform **convolutions** instead of matrix multiplication (as in a linear (aka fully-connected) layer. Please watch (I'd recommend at greater than 1x speed) the following videos to get a basic introduction to convolutional layers and convolutional networks.\n",
    "\n",
    "After watching these videos, you should:\n",
    "* Understand the differences between a standard convolution layer in a convolutional neural network and the traditional 2D image processing convolution we've discussed thus far:\n",
    "    * It's technically cross-correlation instead of convolution (why bother flipping the kernel if the weights are learned?)\n",
    "    * There are Separate weights per channel\n",
    "    * Each filter sums across channels, and produces a single channel of the output feature map\n",
    "* Know the meaning of a few of the variations possible with convolution layers:\n",
    "    * Stride (where the window slides by more than 1 pixel at a time)\n",
    "    * Bias (where a separate scalar parameter is added to the filter's output channel, similar to a bias in a linear layer)\n",
    " \n",
    "Here are the videos:\n",
    "* C4W1L06 - Convolutions Over Volumes https://www.youtube.com/watch?v=KTB_OFoAQcc\n",
    "* C4W1L07 - One Layer of a Convolutional Net https://www.youtube.com/watch?v=jPOAS7uCODQ\n",
    "* C4W1L08 - Simple Convolutional Network Example https://www.youtube.com/watch?v=3PyJA9AfwSk, but don't get bogged down in the notation details.\n",
    "* If you want to see a friendly introduction to Max Pooling layers, also check out this one: C4W1L09 Pooling Layers https://www.youtube.com/watch?v=8oOgPUO-TBY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ1zvCUxhz_Y"
   },
   "source": [
    "# CNN Architecture By Example: LeNet on the MNIST\n",
    "Now, we'll use this notebook to explore the use of CNNs on a \"small\" dataset of handwritten digits. \n",
    "\n",
    "By the end of this activity, you should be able to:\n",
    "* Know some of the typical architectural features of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ1zvCUxhz_Y"
   },
   "source": [
    "An early success case for CNNs was a network called LeNet (after Yann **Le**Cun), which performed well on a dataset of handwritten digits called MNIST.\n",
    "\n",
    "In this notebook you will experiment with different architectures on the MNIST dataset to get a feel for how CNNs work. With modern techniques and compute, this dataset is considered a toy dataset, but it's still an interesting testing ground for architecture ideas. In particular, we're going to look at it through the lens of how many **parameters** we need to learn to do well on the dataset.\n",
    "\n",
    "### You'll Need a GPU\n",
    "\n",
    "To train the models in this notebook, you'll want to be on a machine with an NVIDIA GPU. Please see the [Project 4 instructions](https://facultyweb.cs.wwu.edu/~wehrwes/courses/csci476_24w/p4/#hardware) for details on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHLKy2Ekqoq4"
   },
   "source": [
    "#### Useful functions\n",
    "\n",
    "Run the below cell to define functions that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XPWrOExkqysJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, sys,  math, random, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from io import StringIO\n",
    "import PIL.Image\n",
    "\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if (src_path not in sys.path):\n",
    "    sys.path.insert(0, src_path)\n",
    "    \n",
    "import ML\n",
    "\n",
    "def get_n_params(module):\n",
    "  nparam = 0\n",
    "  for name, param in module.named_parameters():\n",
    "    param_count = 1\n",
    "    for size in list(param.size()):\n",
    "      param_count *= size\n",
    "    nparam += param_count\n",
    "  return nparam\n",
    "\n",
    "def get_model_params(model):\n",
    "  nparam = 0\n",
    "  for name, module in model.named_modules():\n",
    "    nparam += get_n_params(module)\n",
    "  return nparam\n",
    "\n",
    "def to_numpy_image(tensor_or_variable):\n",
    "  \n",
    "  # If this is already a numpy image, just return it\n",
    "  if type(tensor_or_variable) == np.ndarray:\n",
    "    return tensor_or_variable\n",
    "  \n",
    "  # Make sure this is a tensor and not a variable\n",
    "  if type(tensor_or_variable) == Variable:\n",
    "    tensor = tensor_or_variable.data\n",
    "  else:\n",
    "    tensor = tensor_or_variable\n",
    "  \n",
    "  # Convert to numpy and move to CPU if necessary\n",
    "  np_img = tensor.cpu().numpy()\n",
    "  \n",
    "  # If there is no batch dimension, add one\n",
    "  if len(np_img.shape) == 3:\n",
    "    np_img = np_img[np.newaxis, ...]\n",
    "  \n",
    "  # Convert from BxCxHxW (PyTorch convention) to BxHxWxC (OpenCV/numpy convention)\n",
    "  np_img = np_img.transpose(0, 2, 3, 1)\n",
    "  \n",
    "  return np_img\n",
    "\n",
    "def normalize_zero_one_range(tensor_like):\n",
    "  x = tensor_like - tensor_like.min()\n",
    "  x = x / (x.max() + 1e-9)\n",
    "  return x\n",
    "\n",
    "def prep_for_showing(image):\n",
    "  np_img = to_numpy_image(image)\n",
    "  if len(np_img.shape) > 3:\n",
    "    np_img = np_img[0]\n",
    "  np_img = normalize_zero_one_range(np_img)\n",
    "  return np_img\n",
    "\n",
    "def show_image(tensor_var_or_np, title=None, bordercolor=None):\n",
    "  np_img = prep_for_showing(tensor_var_or_np)\n",
    "  \n",
    "  if bordercolor is not None:\n",
    "    np_img = draw_border(np_img, bordercolor)\n",
    "  \n",
    "  # plot it\n",
    "  np_img = np_img.squeeze()\n",
    "  plt.figure(figsize=(4,4))\n",
    "  plt.imshow(np_img)\n",
    "  plt.axis('off')\n",
    "  if title: plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJiMTcx9qr5Z"
   },
   "source": [
    "## Training Data\n",
    "\n",
    "We will use the [MNIST handrwritten digit dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) to train our neural network models. There is a simple wrapper for the MNIST dataset in the torchvision package that implements the Dataset class. We will use that in conjunction with the DataLoader to load training data. Run the below cell to download and initialize our training and test datasets. You should see an example batch of images and their labels shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hURbcBfwqUrY"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "show_image(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j].item() for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has 1 channel (note the channel dimension is before the spatial dimensions), is 28x28 pixels, and our `images` tensor here has a batch of 32 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfN6oCjwrqR1"
   },
   "source": [
    "## Example CNN Architecture\n",
    "\n",
    "Here's a diagram of the LeNet architecture:\n",
    "![](../data/lenet.png)\n",
    "\n",
    "Below is an example model provided to you. This architecture is similar, though not identical to the original LeNet architecture depicted above. It is pretty good and it reaches >98% accuracy on the test set, although better accuracy is quite possible.\n",
    "\n",
    "A few important things to notice about this architecture that are typical of CNN architectures:\n",
    "* The network begins by alternating between:\n",
    "    * conv layers, which keep the spatial dimensions mostly the same, except for the few pixels lost to \"valid\" output size\n",
    "    * Some layer which reduces the spatial resolution.\n",
    "* As the spatial dimensions get smaller, the channel (number of filters, or feature map depth) gets larger\n",
    "* At some point, we start ignoring the spatial dimensions (conceptually \"unrolling the (h x w x c) feature map into a 1D vector of length (h*w*c), then apply some linear (fully-connected) layers.\n",
    "* In this case, the layer that halves the spatial resolution is a 2x2 [max pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layer with stride 2, meaning it takes the max value in every (non-overlapping) 2x2 block of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jjvxIDrXrujd"
   },
   "outputs": [],
   "source": [
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        # Convolution. Input channels: 1, output channels: 6, kernel size: 5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Max-pooling layer that will halve the HxW resolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Another 5x5 convolution that brings channel count up to 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 60)\n",
    "        self.fc2 = nn.Linear(60, 40)\n",
    "        self.fc3 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation and pooling\n",
    "        # Output width after convolution = (input_width - (kernel_size - 1) / 2)\n",
    "        # Output width after pooling = input_width / 2\n",
    "        \n",
    "        # x.size() = Bx1x28x28\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x.size() = Bx6x12x12\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x.size() = Bx16x4x4\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "example_cnn = ExampleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Counting\n",
    "\n",
    "In this exploration, we're going to pay particular attention to the number of parameters in a model - that is, how many weights do we need to store and learn when training the network?\n",
    "\n",
    "Here's the count for the `ExampleModel` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jjvxIDrXrujd"
   },
   "outputs": [],
   "source": [
    "print(f\"Model number of parameters: {get_n_params(example_cnn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the parameter count for our tiny 3-layer MLP from last class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = ML.MLP(28*28, 10) # 28x28 pixel input, 10-class output\n",
    "print(f\"Model number of parameters: {get_n_params(mlp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: 3-layer MLP has a ton of parameters relative to a CNN with more layers! This is because conv layers learn a small number of weights that are applied sliding-window fashion across the entire input, regardless of its spatial dimensions. In contrast, the MLP has \"densely connected\" weights, where everything in one layer depends on everything in the prior layer.\n",
    "\n",
    "Let's look at the relative performance of these two models on MNIST:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NBY2H8yrgRE"
   },
   "source": [
    "## Training Loop\n",
    "The following function trains a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EVzqqVy9rfLk"
   },
   "outputs": [],
   "source": [
    "PRINT_EVERY = 100\n",
    "\n",
    "def train_model(net):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    net.to(device)\n",
    "      \n",
    "    net.train() # set the network in \"training mode\"\n",
    "    \n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss\n",
    "            if i % PRINT_EVERY == PRINT_EVERY - 1:    # print every PRINT_EVERY mini-batches\n",
    "                #show_image(torchvision.utils.make_grid(inputs.data))\n",
    "                print(f\"[{epoch + 1}, {i+1:5d}] loss: {running_loss/100:.3f}\", end=\"\\r\", flush=True)\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0SLImjx1QE1"
   },
   "source": [
    "## Testing\n",
    "\n",
    "The function below evaluates a trained model on the test set. If we were doing this for real, we should only run a model on the test set once, before publishing your results. In this assignment, we're re-using the test set, treating it more like a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jo1Zkn5EyNG7"
   },
   "outputs": [],
   "source": [
    "def test_model(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "    \n",
    "            # if linear_model:\n",
    "            #     images = images.reshape((-1, 28*28))\n",
    "    \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "    \n",
    "        print(f\"# Parameters: {get_n_params(net)}\")\n",
    "        print(f'Accuracy of the network on the 10000 test images: {acc}%')\n",
    "        print(f'Correct: {correct}/{total}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Train the MLP, then test to see its performance.\n",
    "2. Train the example CNN, then test to see its performance.\n",
    "3. (optional; do as much or little as you'd like) Modify the example model to either improve its performance or decrease its parameter count without hurting performance.\n",
    "\n",
    "A few of ideas you can try:\n",
    "* Different model structure (e.g. more layers, smaller/bigger kernels)\n",
    "* Residual connections [0]\n",
    "* Batch [2] / Layer Normalization [3]\n",
    "* Densely connected architectures [1]\n",
    "\n",
    "<font size=\"1em\">[0] https://paperswithcode.com/method/residual-connection</font>\n",
    "<br>\n",
    "<font size=\"1em\">[1] Huang, G., Liu, Z., Weinberger, K. Q., & van der Maaten, L. (2017, July). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 1, No. 2, p. 3).</font>\n",
    "</br>\n",
    "<font size=\"1em\">[2] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.</font>\n",
    "</br>\n",
    "<font size=\"1em\">[3] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP\n",
    "mlp = ML.MLP(28*28, 10) # 28x28 pixel input, 10-class output\n",
    "mlp = train_model(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k2CMdk8LKGdW"
   },
   "outputs": [],
   "source": [
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Define your modules here\n",
    "        ...\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define your dynamic computational graph here\n",
    "        ...\n",
    "        return x\n",
    "\n",
    "# print(f\"Model number of parameters: {get_n_params(mlp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmLl5haX4Om_"
   },
   "source": [
    "### Per-class accuracy\n",
    "\n",
    "Run the below cell to see which digits your model is better at recognizing and which digits it gets confused by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qp8POK0dyOKn"
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (i, 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Solution work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13191424"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227*227*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 864, 2304, 3456, 3456, 54032072704, 16777216, 4096000]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem 6, per-layer parameter counts\n",
    "layers = [f\"c{i}\" for i in range(1,6)] + [f\"d{i}\" for i in range(1,4)]\n",
    "layer_params = [\n",
    "  3*3*3,\n",
    "  3*3*96,\n",
    "  3*3*256,\n",
    "  3*3*384,\n",
    "  3*3*384,\n",
    "  (227*227*256)*4096,\n",
    "  4096*4096,\n",
    "  4096*1000]\n",
    "layer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54052956027"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(layers, layer_params)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CS5670_Project5_MNISTChallenge.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
