{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3109b7e-0092-47a3-aca2-288e745731ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lecture 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0357b50-2962-423e-85f1-80217ec3d134",
   "metadata": {},
   "source": [
    "#### Announcements\n",
    "\n",
    "* Last two faculty candidate talks! Brad McCoy (Computational Topology)\n",
    "    * Thursday 2/1 4pm CF 105 Research Talk: An Invitation to Computational Topology\n",
    "    * Friday 2/2 4pm CF 316 Teaching Demo: Dynamic Programming and Edit Distance\n",
    "* Week 3 Survey - themes\n",
    "    * Notebook presentation is mostly working for folks\n",
    "    * TR class schedule is neutral to good\n",
    "    * One comment I think is relevant: there are no quizzes or anything; how do you know how you're doing?\n",
    "* Reminder: if you submit homework problems late or resubmit, you need to email me if you want me to look at it; please include a changelog (or separate out the problems you want regraded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efd636-c09e-4d1f-a828-33e0fcc75686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Goals\n",
    "\n",
    "* Know the how and why of the MOPS feature descriptor\n",
    "* Know how and why to match features using:\n",
    "  * The SSD metric\n",
    "  * The ratio test\n",
    "* Be able to implement a barebones translational image alignment pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a295613-e6e3-4f35-b603-27b158385acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if (src_path not in sys.path):\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Library imports\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as skim\n",
    "import cv2\n",
    "\n",
    "# codebase imports\n",
    "import util\n",
    "import filtering\n",
    "import features\n",
    "import geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed4ad7-132d-4906-8a67-fae476b90b39",
   "metadata": {},
   "source": [
    "#### Plan\n",
    "\n",
    "* Finish up MOPS description\n",
    "* Feature matching: SSD, ratio distance\n",
    "* Implement a barebones translational image alignment pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d795b83-42fc-467b-a5a1-c9125c68740b",
   "metadata": {},
   "source": [
    "### Panorama Stitching Overview\n",
    "\n",
    "* Detect features\n",
    "* Describe features\n",
    "* Match features\n",
    "* Estimate motion model\n",
    "* Warp image(s) into common coordinate system and blend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1bcb8-4d36-4aa8-8d35-438dcac139ee",
   "metadata": {},
   "source": [
    "#### Finish up MOPS implementation\n",
    "\n",
    "* 576: fill in matrices and do intensity standardization\n",
    "* 476: do intensity standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874380d-d32f-4867-bbfd-2c30d9a5be35",
   "metadata": {},
   "source": [
    "#### Feature Matching\n",
    "\n",
    "So you have a pile of feature descriptors across 2 images - how do we compare them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95260e0f-ec81-4d1f-9349-50e18b2784c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f572b3-5d34-4de1-8afb-9c271db654dd",
   "metadata": {},
   "source": [
    "Simplest metric choice: SSD = sum of squared differences (we used this in the Harris patch error metric)\n",
    "\n",
    "$$ SSD(f, g) = \\sum_{i=1}^d (f_i - g_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a305a0-8b89-481d-8f18-173e840e29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement features.ssd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe8cdf-d952-4691-b61a-f124b3ddd792",
   "metadata": {},
   "source": [
    "Okay, we can compare 2 feautes; how do we find matches?\n",
    "\n",
    "Simplest answer: brute force!\n",
    "\n",
    "##### Homework Problems 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e648d66-d2d6-405d-a784-e7834205f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = np.array([\n",
    "    [0, 1, 4, 3],\n",
    "    [1, 0, 4, 1]], dtype=np.float32)\n",
    "\n",
    "F2 = np.array([\n",
    "    [2, 5, 1],\n",
    "    [1, 5, 2]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9314e-2f36-4729-a57f-cd22f9cd2341",
   "metadata": {},
   "source": [
    "##### Homework Problems 1-2\n",
    "You can now do these with code or by hand:\n",
    "\n",
    "1. Create a table with 4 rows and 3 columns in which the $(i,j)$th cell contains the SSD distance between feature $i$ in F1 and feature $j$ in F2.\n",
    "\n",
    "2. For each feature in F1, give the **index** of the closest feature match in F2 using the SSD metric.\n",
    "\n",
    "   *Note: The homework problem asks for 1-indexed indices, so don't forget to add 1 if you're coding this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020c85cd-b31f-4a75-969b-30ad6b2ea4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. 41.  2.]\n",
      " [ 2. 41.  4.]\n",
      " [13.  2. 13.]\n",
      " [ 1. 20.  5.]]\n",
      "[3 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "d, n1 = F1.shape\n",
    "n2 = F2.shape[1]\n",
    "\n",
    "distances = np.zeros((4, 3))\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        distances[i,j] = features.ssd(F1[:,i], F2[:,j])\n",
    "\n",
    "print(distances)\n",
    "print(np.argmin(distances, axis=1)+1)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b1290-69d4-4929-b202-5f85bb165c79",
   "metadata": {},
   "source": [
    "\n",
    "Optimized brute force: [scipy.spatial.distance.cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) vectorizes the brute force for you; we're using the `sqeuclidean` distance.\n",
    "\n",
    "You can get fancy by using spatial data structures like kd-trees, etc\n",
    "* This is basically fast nearest neighbor search; so hot right now\n",
    "* Fast approximate nearest neighbor search is also a big thing, but may not be great for us (you'll see why soon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6542f34-66f9-4ec6-b49a-f46f6d7a14ed",
   "metadata": {},
   "source": [
    "#### Algorithm to get a list of feature correspondences:\n",
    "*  Algorithm:\n",
    "  * foreach feature in 1\n",
    "    * Take the closest feature in 2\n",
    "   \n",
    "Problem: the closest thing may not be very close\n",
    "\n",
    "Solution: threshold!\n",
    "\n",
    "*  Algorithm, take 2:\n",
    "  * foreach feature in 1\n",
    "    * Take the closest feature in 2\n",
    "    * add them to a list of correspondences if their distance is less than threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21660538-5ed4-46bf-b672-4fed98c4a3c5",
   "metadata": {},
   "source": [
    "#### The Fence Strikes Back\n",
    "\n",
    "![](../data/fences.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86350739-fdae-487e-8d64-cb3482a6ba25",
   "metadata": {},
   "source": [
    "Insight: good matches are ones where the closest thing isn't just *barely* the closest thing.\n",
    "\n",
    "Idea: look at the *second* closest match. Specifically, if $g_1$ and $g_2$ are the closest and second closest matches in image 2 to $f$ in image 1, then\n",
    "\n",
    "$$\n",
    "d_{\\mathrm{ratio}} = \\frac{SSD(f, g1)}{SSD(f, g2)}\n",
    "$$\n",
    "\n",
    "What does this equal if $g1$ and $g2$ are equally far from $f$?\n",
    "\n",
    "How does it behave as $g_2$ gets increasingly far from $f$ compared to $g_1$? \n",
    "\n",
    "\n",
    "##### HW Problem 3\n",
    "Again, you can do this by hand or with code:\n",
    "\n",
    "```python\n",
    "F1 =[0, 1, 4, 3],\n",
    "    [1, 0, 4, 1]], dtype=np.float32)\n",
    "\n",
    "F2 = np.array([\n",
    "    [2, 5, 1],\n",
    "    [1, 5, 2]], dtype=np.float32)\n",
    "```\n",
    "\n",
    "For each feature in F2, give the index of the closest feature match in F1 **and** the ratio distance between each feature and its closest match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57206b81-51ac-4444-8758-2e81dd5f257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(distances, axis=0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098029af-8434-4045-83bf-8ab2e104bf62",
   "metadata": {},
   "source": [
    "```\n",
    "[[ 4. 41.  2.]\n",
    " [ 2. 41.  4.]\n",
    " [13.  2. 13.]\n",
    " [ 1. 20.  5.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5b721-c5de-4525-9675-663ca166a78a",
   "metadata": {},
   "source": [
    "#### We have (almost all) the pieces\n",
    "\n",
    "of an end-to-end image stitching pipeline. Two missing bits:\n",
    "\n",
    "1. How do we model the motion between two images?\n",
    "\n",
    "2. How do we get the images into a common coordinate system and blend them?\n",
    "\n",
    "\n",
    "#### Bit 1:\n",
    "\n",
    "For some simple cases (specifically: long focal length cameras), a very good approximation the motion from one image in a panorama sequence to another is a simple **translation**. That is, the image can be (pretty much) aligned by simply offseting all the pixels by some amount in $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bcf38-03a1-442a-9f78-0865bd85f356",
   "metadata": {},
   "source": [
    "**Brainstorm**: Given a list of feature correspondences, how could we estimate a single translation model that fits them as well as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cea333-bfb1-4c7a-a698-d109dce23191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59275c0d-355b-4d93-a59d-f7453ca812e4",
   "metadata": {},
   "source": [
    "Sensible-seeming approach: average the displacements! If $(\\mathbf{f}_i, \\mathbf{g}_i)$ are corresponding feature pairs in image 1 and 2, then\n",
    "\n",
    "$$\\mathbf{t} = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{f}_i - \\mathbf{g}_i)$$\n",
    "\n",
    "We'll return to this later and find that this not only seems sensible, but is in fact, a principled approach for a translational motion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6618d2-642b-4383-9771-4d62d92e27d1",
   "metadata": {},
   "source": [
    "#### Bit 2:\n",
    "\n",
    "Build an affine transformation matrix and warp image 2 into an image that's large enough to fit its extent.\n",
    "\n",
    "##### Homework Problems 4-5\n",
    "\n",
    "4. Give a 3x3 affine transformation matrix that can be used to warp image 2 into image 1's coordinates.\n",
    "\n",
    "5. If image 1's origin is at its top left and $t_x$ and $t_y$​ are both positive, what's the size of the destination image that can contain the combined image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f96a8d-95f5-402e-a3ac-1b4b487f079a",
   "metadata": {},
   "source": [
    "\n",
    "### Can we code this up in the time remaining?\n",
    "\n",
    "(I have no idea whether this is doable, or if it will work well)\n",
    "\n",
    "TODO:\n",
    "* find Harris keypoints (use `features.harris_corners` then `features.get_harris_points`)\n",
    "* extract descriptors (use `features.extract_MOPS`)\n",
    "* implement `features.compute_distances` (use `features.ssd` and for loops, or use `scipy.spatial.distance.cdist`)\n",
    "* implement `features.get_matches` to find closest matches and threshold by match score \n",
    "  * For SSD, `np.argmin` should do\n",
    "  * For ratio distance, `np.argsort` is probably where it's at\n",
    "* implement `geometry.estimate_translation` to average the differences between correspondences\n",
    "* build an affine transformation matrix that applies that translation\n",
    "* warp image 2 into a new image that's large enough to fit both (use `geometry.warp`)\n",
    "* add (average? blend somehow?) image 1 into the warped image 2\n",
    "\n",
    "If you get this working (i.e., running - it may not find a good model!), then try making it multi-scale:\n",
    "* compute a Gaussian pyramid (implement `filtering.gaussian_pyramid`)\n",
    "* adjust the above feature detection, extraction, and transformation estimation steps to account for multiple scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418a817-5bf1-4bcc-94df-95362c87530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are two images where a translational alignment should work\n",
    "y1 = imageio.imread(\"../data/yos1.jpg\").astype(np.float32) / 255\n",
    "y1 = skim.color.rgb2gray(y1)\n",
    "\n",
    "y2 = imageio.imread(\"../data/yos2.jpg\").astype(np.float32) / 255\n",
    "y2 = skim.color.rgb2gray(y2)\n",
    "\n",
    "util.imshow_gray(y1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
